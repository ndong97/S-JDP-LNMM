{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import matmul\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from numpy.linalg import inv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, chi2\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_censored, cumulative_dynamic_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter(string, substr):\n",
    "    return [str for str in string if\n",
    "             any(sub in str for sub in substr)]\n",
    "\n",
    "def vectorized_gaussian_logpdf(X, means, covariances):\n",
    "    # \"\"\"\n",
    "    # Compute log N(x_i; mu_i, sigma_i) for each x_i, mu_i, sigma_i\n",
    "    # Args:\n",
    "    #     X : shape (d)\n",
    "    #         Data points\n",
    "    #     means : shape (self.N, d)\n",
    "    #         Mean vectors\n",
    "    #     covariances : shape (self.N, d)\n",
    "    #         Diagonal covariance matrices\n",
    "    # Returns:\n",
    "    #     logpdfs : shape (n,)\n",
    "    #         Log probabilities\n",
    "    # \"\"\"\n",
    "        d = X.shape[0]\n",
    "        constant = d * np.log(2 * np.pi)\n",
    "        log_determinants = np.log(np.prod(covariances, axis=1))\n",
    "        deviations = X - means\n",
    "        inverses = 1 / covariances\n",
    "        return -0.5 * (constant + log_determinants + np.sum(deviations * inverses * deviations, axis=1))\n",
    "\n",
    "def vectorized_norm_loglikelihood(X, means, covariances, delta):\n",
    "    #  '''\n",
    "    #  Compute log Norm(X_i, mu_i, sigma_i) for each x_i, mu_i, sigma_i\n",
    "    #  Args:\n",
    "    #      X: shape (n,)\n",
    "    #      means: shape (n,d)\n",
    "    #      covariances (d,)\n",
    "    #  Returns: logsf: shape (d,)\n",
    "    #  '''\n",
    "    loglikelihood = [delta*np.log(stats.norm.pdf(x = X, loc=means[:, i], scale=np.sqrt(covariances[i]))) + (1-delta)*np.log(stats.norm.sf(x = X, loc=means[:, i], scale=np.sqrt(covariances[i]))) for i in range(means.shape[1])]\n",
    "    loglikelihood = np.array(loglikelihood).sum(axis=1)\n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_Y = []\n",
    "Z_X = []\n",
    "BETA_Y = []\n",
    "TAU_Y = []\n",
    "BETA_X = []\n",
    "TAU_X = []\n",
    "data_train = []\n",
    "ind_train = [0,1,2,3,4,5,6,8,9,10,11,12,13,14,15,16]\n",
    "ind_test = [7,17,18,19]\n",
    "for i in ind_train:\n",
    "    dat = pd.read_csv(f'C:/Users/ndong/OneDrive - Texas Tech University/Documents/Scalable Survival-DP Package/Simulation/Original Copy of Simulation/Simulation 3 - small std/simulated data 3_{i+1}.csv', index_col=0)\n",
    "    z_y = pd.read_csv(f'C:/Users/ndong/OneDrive - Texas Tech University/Documents/Scalable Survival-DP Package/Simulation/Original Copy of Simulation/Simulation 3 - small std/3_{i+1}_results/simulation_3_{i+1}_Z_Y_records.csv', index_col=0)\n",
    "    z_x = pd.read_csv(f'C:/Users/ndong/OneDrive - Texas Tech University/Documents/Scalable Survival-DP Package/Simulation/Original Copy of Simulation/Simulation 3 - small std/3_{i+1}_results/simulation_3_{i+1}_Z_X_records.csv', index_col=0)\n",
    "    beta_y = pd.read_csv(f'C:/Users/ndong/OneDrive - Texas Tech University/Documents/Scalable Survival-DP Package/Simulation/Original Copy of Simulation/Simulation 3 - small std/3_{i+1}_results/simulation_3_{i+1}_BETA_Y_records.csv', index_col=0)\n",
    "    tau_y = pd.read_csv(f'C:/Users/ndong/OneDrive - Texas Tech University/Documents/Scalable Survival-DP Package/Simulation/Original Copy of Simulation/Simulation 3 - small std/3_{i+1}_results/simulation_3_{i+1}_TAU_Y_records.csv', index_col=0)\n",
    "    beta_x = pd.read_csv(f'C:/Users/ndong/OneDrive - Texas Tech University/Documents/Scalable Survival-DP Package/Simulation/Original Copy of Simulation/Simulation 3 - small std/3_{i+1}_results/simulation_3_{i+1}_BETA_X_records.csv', index_col=0)\n",
    "    tau_x = pd.read_csv(f'C:/Users/ndong/OneDrive - Texas Tech University/Documents/Scalable Survival-DP Package/Simulation/Original Copy of Simulation/Simulation 3 - small std/3_{i+1}_results/simulation_3_{i+1}_TAU_X_records.csv', index_col=0)\n",
    "    Z_Y.append(z_y)\n",
    "    Z_X.append(z_x)\n",
    "    BETA_Y.append(beta_y)\n",
    "    TAU_Y.append(tau_y)\n",
    "    BETA_X.append(beta_x)\n",
    "    TAU_X.append(tau_x)\n",
    "    data_train.append(dat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=[]\n",
    "for i in ind_test:\n",
    "    dat = pd.read_csv(f'C:/Users/ndong/OneDrive - Texas Tech University/Documents/Scalable Survival-DP Package/Simulation/Original Copy of Simulation/Simulation 3 - small std/simulated data 3_{i+1}.csv', index_col=0)\n",
    "    data_test.append(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training = pd.concat(data_train, ignore_index=True)\n",
    "true_train_z_y = Training['z_y']\n",
    "true_train_y = Training['Y']\n",
    "Testing = pd.concat(data_test, ignore_index=True)\n",
    "true_test_z_y = Testing['z_y']\n",
    "true_test_y = Testing['Y']\n",
    "delta_test = Testing['c_y']\n",
    "Testing1 = Testing.drop(columns=['z_x', 'z_y', 'c_y', 'y_mu', 'y_sigma', 'Y'])\n",
    "X_name = Testing1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agg_z_y = [[Z_Y[i].copy() for i in range(16)] for j in range(50)]\n",
    "# Agg_z_x = [[Z_X[i].copy() for i in range(16)] for j in range(50)]\n",
    "# Agg_beta_y = [[BETA_Y[i].copy() for i in range(16)] for j in range(50)]\n",
    "# Agg_tau_y = [[TAU_Y[i].copy() for i in range(16)] for j in range(50)]\n",
    "\n",
    "# n_subsets_cluster_y = [[Z_Y[j].iloc[:,i].nunique() for j in range(16)] for i in range(50)]\n",
    "# print(n_subsets_cluster_y)\n",
    "# n_subsets_cluster_x = [[Z_X[j].iloc[:,i].nunique() for j in range(16)] for i in range(50)]\n",
    "# random.seed(120)\n",
    "# reference_subset = random.choices(range(16),k=50)\n",
    "# print(reference_subset)\n",
    "\n",
    "# for t in range(50):\n",
    "#     i = reference_subset[t]\n",
    "#     print(i)\n",
    "#     for j in range(50):\n",
    "#         target_z_y = Z_Y[i].iloc[:,j]\n",
    "#         n_N_y = target_z_y.value_counts(sort=False).sort_index()\n",
    "#         target_z_x = Z_X[i].iloc[:,j]\n",
    "#         beta_y = BETA_Y[i]\n",
    "#         name_beta_y = Filter(beta_y.columns, [f'in_{j+1}_iteration'])\n",
    "#         target_beta_y = beta_y[name_beta_y].values\n",
    "#         tau_y = TAU_Y[i]\n",
    "#         name_tau_y = Filter(tau_y.columns, [f'in_{j+1}_iteration'])\n",
    "#         target_tau_y = tau_y[name_tau_y].values\n",
    "#         beta_x = BETA_X[i]\n",
    "#         name_beta_x = Filter(beta_x.columns, [f'in_{j+1}_iteration'])\n",
    "#         target_beta_x = beta_x[name_beta_x].values\n",
    "#         tau_x = TAU_X[i]\n",
    "#         name_tau_x = Filter(tau_x.columns, [f'in_{j+1}_iteration'])\n",
    "#         target_tau_x = tau_x[name_tau_x].values\n",
    "#         for k in range(16):\n",
    "#             n_n = n_subsets_cluster_y[j][k]\n",
    "#             print(f'the {k+1}th subset in the {j+1}th iteration')\n",
    "#             if k != i:\n",
    "#                 DAT = data_train[k]\n",
    "#                 for s in range(n_n):\n",
    "#                     ddd = DAT[Z_Y[k][f'z_y_{j+1}_iteration'] == s]\n",
    "#                     print(ddd.shape)\n",
    "#                     X = ddd[X_name].values\n",
    "#                     delta = ddd['c_y'].values\n",
    "#                     y = ddd['Y'].values\n",
    "#                     print(y.shape)\n",
    "#                     mean_y = matmul(X, target_beta_y)\n",
    "#                     print(mean_y.shape)\n",
    "#                     likelihood_y = vectorized_norm_loglikelihood(y, mean_y, target_tau_y[0], delta)\n",
    "#                     print(likelihood_y)\n",
    "#                     likelihood_y = np.nan_to_num(likelihood_y, nan=-np.inf)\n",
    "#                     likelihood_x = [np.exp(vectorized_gaussian_logpdf(X[t], target_beta_x.T, target_tau_x.T)) for t in range(X.shape[0])]\n",
    "#                     likelihood_x = np.array(likelihood_x).sum(axis=0)\n",
    "#                     true_z_y = np.argmax(likelihood_y)\n",
    "#                     print(true_z_y)\n",
    "#                     true_z_x = np.argmax(likelihood_x)\n",
    "#                     Agg_z_y[t][k].loc[ddd.index, f'z_y_{j+1}_iteration'] = true_z_y\n",
    "#                     Agg_z_x[t][k].loc[ddd.index, f'z_x_{j+1}_iteration'] = true_z_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agg_z_y = [Z_Y[i].copy() for i in range(16)]\n",
    "Agg_z_x = [Z_X[i].copy() for i in range(16)]\n",
    "\n",
    "n_subsets_cluster_y = [[Z_Y[j].iloc[:,i].nunique() for j in range(16)] for i in range(50)]\n",
    "# print(n_subsets_cluster_y)\n",
    "n_subsets_cluster_x = [[Z_X[j].iloc[:,i].nunique() for j in range(16)] for i in range(50)]\n",
    "random.seed(10)\n",
    "reference_subset = random.choices(range(16),k=50)\n",
    "# print(reference_subset)\n",
    "for t in range(50):\n",
    "    i = reference_subset[t]\n",
    "    # print(i)\n",
    "    target_z_y = Z_Y[i].iloc[:,t]\n",
    "    n_N_y = target_z_y.value_counts(sort=False).sort_index()\n",
    "    target_z_x = Z_X[i].iloc[:,t]\n",
    "    beta_y = BETA_Y[i]\n",
    "    name_beta_y = Filter(beta_y.columns, [f'in_{t+1}_iteration'])\n",
    "    target_beta_y = beta_y[name_beta_y].values\n",
    "    tau_y = TAU_Y[i]\n",
    "    name_tau_y = Filter(tau_y.columns, [f'in_{t+1}_iteration'])\n",
    "    target_tau_y = tau_y[name_tau_y].values\n",
    "    beta_x = BETA_X[i]\n",
    "    name_beta_x = Filter(beta_x.columns, [f'in_{t+1}_iteration'])\n",
    "    target_beta_x = beta_x[name_beta_x].values\n",
    "    tau_x = TAU_X[i]\n",
    "    name_tau_x = Filter(tau_x.columns, [f'in_{t+1}_iteration'])\n",
    "    target_tau_x = tau_x[name_tau_x].values\n",
    "    for k in range(16):\n",
    "        n_n = n_subsets_cluster_y[t][k]\n",
    "        # print(f'the {k+1}th subset in the {t+1}th iteration')\n",
    "        if k != i:\n",
    "            DAT = data_train[k]\n",
    "            for s in range(n_n):\n",
    "                ddd = DAT[Z_Y[k][f'z_y_{t+1}_iteration'] == s]\n",
    "                # print(ddd.shape)\n",
    "                X = ddd[X_name].values\n",
    "                delta = ddd['c_y'].values\n",
    "                y = ddd['Y'].values\n",
    "                # print(y.shape)\n",
    "                mean_y = matmul(X, target_beta_y)\n",
    "                # print(mean_y.shape)\n",
    "                likelihood_y = vectorized_norm_loglikelihood(y, mean_y, target_tau_y[0], delta)\n",
    "                # print(likelihood_y)\n",
    "                likelihood_y = np.nan_to_num(likelihood_y, nan=-np.inf)\n",
    "                likelihood_x = [np.exp(vectorized_gaussian_logpdf(X[t], target_beta_x.T, target_tau_x.T)) for t in range(X.shape[0])]\n",
    "                likelihood_x = np.array(likelihood_x).sum(axis=0)\n",
    "                true_z_y = np.argmax(likelihood_y)\n",
    "                # print(true_z_y)\n",
    "                true_z_x = np.argmax(likelihood_x)\n",
    "                Agg_z_y[k].loc[ddd.index, f'z_y_{t+1}_iteration'] = true_z_y\n",
    "                Agg_z_x[k].loc[ddd.index, f'z_x_{t+1}_iteration'] = true_z_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RI = []\n",
    "train_MI = []\n",
    "train_ARI = []\n",
    "train_AMI = []\n",
    "train_HOMO = []\n",
    "train_COM = []\n",
    "train_VM = []\n",
    "train_FM = []\n",
    "AGG_Z_Y = pd.concat(Agg_z_y, ignore_index=True)\n",
    "for j in range(AGG_Z_Y.shape[1]):\n",
    "    ri = metrics.rand_score(true_train_z_y, AGG_Z_Y.iloc[:,j])\n",
    "    mi = metrics.mutual_info_score(true_train_z_y, AGG_Z_Y.iloc[:,j])\n",
    "    ari = metrics.adjusted_rand_score(true_train_z_y,AGG_Z_Y.iloc[:,j])\n",
    "    ami = metrics.adjusted_mutual_info_score(true_train_z_y, AGG_Z_Y.iloc[:,j])\n",
    "    hcv = metrics.homogeneity_completeness_v_measure(true_train_z_y, AGG_Z_Y.iloc[:,j])\n",
    "    fm = metrics.fowlkes_mallows_score(true_train_z_y, AGG_Z_Y.iloc[:,j])\n",
    "    train_RI.append(ri)\n",
    "    train_MI.append(mi)\n",
    "    train_ARI.append(ari)\n",
    "    train_AMI.append(ami)\n",
    "    train_HOMO.append(hcv[0])\n",
    "    train_COM.append(hcv[1])\n",
    "    train_VM.append(hcv[2])\n",
    "    train_FM.append(fm)\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train-RI</th>\n",
       "      <th>Train-MI</th>\n",
       "      <th>Train-ARI</th>\n",
       "      <th>Train-AMI</th>\n",
       "      <th>Train-HOMO</th>\n",
       "      <th>Train-COM</th>\n",
       "      <th>Train-VM</th>\n",
       "      <th>Train-FM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.860236</td>\n",
       "      <td>0.781942</td>\n",
       "      <td>0.704578</td>\n",
       "      <td>0.745203</td>\n",
       "      <td>0.711771</td>\n",
       "      <td>0.790145</td>\n",
       "      <td>0.745267</td>\n",
       "      <td>0.818175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.060733</td>\n",
       "      <td>0.111932</td>\n",
       "      <td>0.114162</td>\n",
       "      <td>0.064951</td>\n",
       "      <td>0.101887</td>\n",
       "      <td>0.033299</td>\n",
       "      <td>0.064932</td>\n",
       "      <td>0.058702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train-RI  Train-MI  Train-ARI  Train-AMI  Train-HOMO  Train-COM  \\\n",
       "mean  0.860236  0.781942   0.704578   0.745203    0.711771   0.790145   \n",
       "std   0.060733  0.111932   0.114162   0.064951    0.101887   0.033299   \n",
       "\n",
       "      Train-VM  Train-FM  \n",
       "mean  0.745267  0.818175  \n",
       "std   0.064932  0.058702  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Train-RI': [np.array(train_RI).mean(), np.array(train_RI).std()],'Train-MI': [np.array(train_MI).mean(), np.array(train_MI).std()],'Train-ARI': [np.array(train_ARI).mean(), np.array(train_ARI).std()], 'Train-AMI': [np.array(train_AMI).mean(), np.array(train_AMI).std()],'Train-HOMO': [np.array(train_HOMO).mean(), np.array(train_HOMO).std()], 'Train-COM': [np.array(train_COM).mean(), np.array(train_COM).std()], 'Train-VM': [np.array(train_VM).mean(), np.array(train_VM).std()], 'Train-FM': [np.array(train_FM).mean(), np.array(train_FM).std()]}, index=['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clustering:\n",
    "Agg_z_y_test = pd.DataFrame(np.ones((Testing1.shape[0], 50)), columns=Z_Y[0].columns)\n",
    "Agg_z_x_test = pd.DataFrame(np.ones((Testing1.shape[0], 50)), columns=Z_X[0].columns)\n",
    "print(len(n_subsets_cluster_y))\n",
    "for t in range(50):\n",
    "    i = reference_subset[t]\n",
    "    n_n = n_subsets_cluster_y[t][i]\n",
    "    print(n_n)\n",
    "    target_z_y = Z_Y[i].iloc[:,t]\n",
    "    n_N_y = target_z_y.value_counts(sort=False).sort_index()\n",
    "    target_z_x = Z_X[i].iloc[:,t]\n",
    "    beta_y = BETA_Y[i]\n",
    "    name_beta_y = Filter(beta_y.columns, [f'in_{t+1}_iteration'])\n",
    "    target_beta_y = beta_y[name_beta_y].values\n",
    "    target_beta_y = target_beta_y[:,:n_n]\n",
    "    tau_y = TAU_Y[i]\n",
    "    name_tau_y = Filter(tau_y.columns, [f'in_{t+1}_iteration'])\n",
    "    target_tau_y = tau_y[name_tau_y].values\n",
    "    target_tau_y = target_tau_y[:,:n_n]\n",
    "    beta_x = BETA_X[i]\n",
    "    name_beta_x = Filter(beta_x.columns, [f'in_{t+1}_iteration'])\n",
    "    target_beta_x = beta_x[name_beta_x].values\n",
    "    tau_x = TAU_X[i]\n",
    "    name_tau_x = Filter(tau_x.columns, [f'in_{t+1}_iteration'])\n",
    "    target_tau_x = tau_x[name_tau_x].values\n",
    "    n_N_x = []\n",
    "    print(target_beta_y.shape, target_beta_x.shape)\n",
    "    for s in range(n_subsets_cluster_x[t][i]):\n",
    "        group = target_z_y[target_z_x == s]\n",
    "        n_m = np.zeros(n_N_y.shape[0])\n",
    "        n_m_effective = group.value_counts(sort=False).sort_index()\n",
    "        n_m[n_m_effective.index] = n_m_effective\n",
    "        n_N_x.append(n_m)\n",
    "    n_N_x = np.array(n_N_x).T\n",
    "    print(n_N_x)\n",
    "    for j in range(Testing1.shape[0]):\n",
    "        y = true_test_y[j]\n",
    "        x = Testing1.loc[j, ].values\n",
    "        delta = delta_test[j]\n",
    "        mean_y = matmul(x.reshape(1,-1), target_beta_y)\n",
    "        likelihood_x = vectorized_gaussian_logpdf(x, target_beta_x.T, target_tau_x.T)\n",
    "        pred_z_x = np.argmax(likelihood_x)\n",
    "        # ind_x = []\n",
    "        # for k in range(n_n):\n",
    "        #     zzx = target_z_x[target_z_y == k]\n",
    "        #     ind_x.append(zzx.iloc[0])\n",
    "        likelihood_x = np.exp(likelihood_x)\n",
    "        # print(likelihood_x.shape)\n",
    "        likelihood_x = matmul(n_N_x, likelihood_x)\n",
    "        likelihood_x = likelihood_x[:n_n]\n",
    "        # print(likelihood_x.shape)\n",
    "        likelihood_y = (stats.norm.pdf(y, loc=mean_y[0], scale=np.sqrt(target_tau_y[0]))**delta) * (stats.norm.sf(y, loc=mean_y[0], scale=np.sqrt(target_tau_y[0]))**(1-delta))\n",
    "        pred_z_y = np.argmax(likelihood_y*likelihood_x)\n",
    "        # print(np.argmax(likelihood_y+likelihood_x), true_test_z_y[j])\n",
    "        Agg_z_y_test.iloc[j, t] = pred_z_y\n",
    "        Agg_z_x_test.iloc[j, t] = pred_z_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RI = []\n",
    "test_MI = []\n",
    "test_ARI = []\n",
    "test_AMI = []\n",
    "test_HOMO = []\n",
    "test_COM = []\n",
    "test_VM = []\n",
    "test_FM = []\n",
    "for j in range(43):\n",
    "    ri = metrics.rand_score(true_test_z_y, Agg_z_y_test.iloc[:,j])\n",
    "    mi = metrics.mutual_info_score(true_test_z_y, Agg_z_y_test.iloc[:,j])\n",
    "    ari = metrics.adjusted_rand_score(true_test_z_y,Agg_z_y_test.iloc[:,j])\n",
    "    ami = metrics.adjusted_mutual_info_score(true_test_z_y, Agg_z_y_test.iloc[:,j])\n",
    "    hcv = metrics.homogeneity_completeness_v_measure(true_test_z_y, Agg_z_y_test.iloc[:,j])\n",
    "    fm = metrics.fowlkes_mallows_score(true_test_z_y, Agg_z_y_test.iloc[:,j])\n",
    "    test_RI.append(ri)\n",
    "    test_MI.append(mi)\n",
    "    test_ARI.append(ari)\n",
    "    test_AMI.append(ami)\n",
    "    test_HOMO.append(hcv[0])\n",
    "    test_COM.append(hcv[1])\n",
    "    test_VM.append(hcv[2])\n",
    "    test_FM.append(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test-RI</th>\n",
       "      <th>Test-MI</th>\n",
       "      <th>Test-ARI</th>\n",
       "      <th>Test-AMI</th>\n",
       "      <th>Test-HOMO</th>\n",
       "      <th>Test-COM</th>\n",
       "      <th>Test-VM</th>\n",
       "      <th>Test-FM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.902335</td>\n",
       "      <td>0.905221</td>\n",
       "      <td>0.785678</td>\n",
       "      <td>0.832384</td>\n",
       "      <td>0.823992</td>\n",
       "      <td>0.842278</td>\n",
       "      <td>0.832544</td>\n",
       "      <td>0.861297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.059464</td>\n",
       "      <td>0.118482</td>\n",
       "      <td>0.129125</td>\n",
       "      <td>0.093650</td>\n",
       "      <td>0.107850</td>\n",
       "      <td>0.081343</td>\n",
       "      <td>0.093562</td>\n",
       "      <td>0.082607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test-RI   Test-MI  Test-ARI  Test-AMI  Test-HOMO  Test-COM   Test-VM  \\\n",
       "mean  0.902335  0.905221  0.785678  0.832384   0.823992  0.842278  0.832544   \n",
       "std   0.059464  0.118482  0.129125  0.093650   0.107850  0.081343  0.093562   \n",
       "\n",
       "       Test-FM  \n",
       "mean  0.861297  \n",
       "std   0.082607  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Test-RI': [np.array(test_RI).mean(), np.array(test_RI).std()],'Test-MI': [np.array(test_MI).mean(), np.array(test_MI).std()],'Test-ARI': [np.array(test_ARI).mean(), np.array(test_ARI).std()], 'Test-AMI': [np.array(test_AMI).mean(), np.array(test_AMI).std()],'Test-HOMO': [np.array(test_HOMO).mean(), np.array(test_HOMO).std()], 'Test-COM': [np.array(test_COM).mean(), np.array(test_COM).std()], 'Test-VM': [np.array(test_VM).mean(), np.array(test_VM).std()], 'Test-FM': [np.array(test_FM).mean(), np.array(test_FM).std()]}, index=['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agg_beta_y = [BETA_Y[i].copy() for i in range(16)]\n",
    "Agg_tau_y = [TAU_Y[i].copy() for i in range(16)]\n",
    "\n",
    "\n",
    "for t in range(50):\n",
    "    i = reference_subset[t]\n",
    "    for k in range(16):\n",
    "        n_n = n_subsets_cluster_y[t][k]\n",
    "        if k != i:\n",
    "            for s in range(n_n): #s is the label in original subsets\n",
    "                ind = Z_Y[k][Z_Y[k][f'z_y_{t+1}_iteration'] == s].index\n",
    "                agged_z_y = Agg_z_y[k].loc[ind[0], f'z_y_{t+1}_iteration']\n",
    "                Agg_beta_y[k][f'beta_y_cluster{agged_z_y+1}_in_{t+1}_iteration'] = BETA_Y[k][f'beta_y_cluster{s+1}_in_{t+1}_iteration']\n",
    "                Agg_tau_y[k][f'tau_y_cluster{agged_z_y+1}_in_{t+1}_iteration'] = TAU_Y[k][f'tau_y_cluster{s+1}_in_{t+1}_iteration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####To be rewrite\n",
    "\n",
    "\n",
    "Average_beta_over_iterations_name = BETA_Y[0].columns\n",
    "Average_beta_over_iterations = []\n",
    "Average_tau_over_iterations_name = TAU_Y[0].columns\n",
    "Average_tau_over_iterations = []\n",
    "for i in range(50):\n",
    "    # alter_beta_over_iter = []\n",
    "    # alter_tau_over_iter = []\n",
    "    for k in range(3):\n",
    "        alter_beta_over_iter_cluster = []\n",
    "        alter_tau_over_iter_cluster = []\n",
    "        for j in range(16):\n",
    "            alter_beta_over_iter_cluster.append(Agg_beta_y[j][f'beta_y_cluster{k+1}_in_{i+1}_iteration'])\n",
    "            alter_tau_over_iter_cluster.append(Agg_tau_y[j][f'tau_y_cluster{k+1}_in_{i+1}_iteration'])\n",
    "        alter_beta_over_iter_cluster = np.array(alter_beta_over_iter_cluster).mean(axis=0)\n",
    "        alter_tau_over_iter_cluster = np.array(alter_tau_over_iter_cluster).mean()\n",
    "        Average_beta_over_iterations.append(alter_beta_over_iter_cluster)\n",
    "        Average_tau_over_iterations.append(alter_tau_over_iter_cluster)\n",
    "Average_beta_along_iterations = pd.DataFrame(np.array(Average_beta_over_iterations).T, columns=Average_beta_over_iterations_name)\n",
    "Average_tau_along_iterations = pd.DataFrame(np.array(Average_tau_over_iterations).reshape(1,-1), columns=Average_tau_over_iterations_name)\n",
    "print(Average_beta_along_iterations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Average_beta_over_subsets_name = []\n",
    "Average_beta_over_subsets = []\n",
    "Average_tau_over_subsets_name = []\n",
    "Average_tau_over_subsets = []\n",
    "for i in ind_train:\n",
    "    for j in range(3):\n",
    "        Average_beta_over_subsets_name.append(f'beta_y_cluster{j+1}_in_{i+1}_subset')\n",
    "        Average_tau_over_subsets_name.append(f'tau_y_cluster{j+1}_in_{i+1}_subset')\n",
    "\n",
    "for i in range(16):\n",
    "    # alter_beta_over_iter = []\n",
    "    # alter_tau_over_iter = []\n",
    "    for k in range(3):\n",
    "        alter_beta_over_sub_cluster = []\n",
    "        alter_tau_over_sub_cluster = []\n",
    "        for j in range(50):\n",
    "            alter_beta_over_sub_cluster.append(Agg_beta_y[i][f'beta_y_cluster{k+1}_in_{j+1}_iteration'])\n",
    "            alter_tau_over_sub_cluster.append(Agg_tau_y[i][f'tau_y_cluster{k+1}_in_{j+1}_iteration'])\n",
    "        alter_beta_over_sub_cluster = np.array(alter_beta_over_iter_cluster).mean(axis=0)\n",
    "        alter_tau_over_sub_cluster = np.array(alter_tau_over_iter_cluster).mean()\n",
    "        Average_beta_over_subsets.append(alter_beta_over_iter_cluster)\n",
    "        Average_tau_over_subsets.append(alter_tau_over_iter_cluster)\n",
    "Average_beta_along_subsets = pd.DataFrame(np.array(Average_beta_over_subsets).T, columns=Average_beta_over_subsets_name)\n",
    "Average_tau_along_subsets = pd.DataFrame(np.array(Average_tau_over_subsets).reshape(1,-1), columns=Average_tau_over_subsets_name)\n",
    "print(Average_beta_along_subsets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 50)\n"
     ]
    }
   ],
   "source": [
    "preds_time_stamp = [365*i for i in [1, 2, 3, 5, 7, 9]]\n",
    "SCORE_train = []\n",
    "SCORE_test = []\n",
    "PRC_train = []\n",
    "PRC_test = []\n",
    "\n",
    "for k in preds_time_stamp:\n",
    "    Score_train = []\n",
    "    Score_test = []\n",
    "    Prc_train = []\n",
    "    Prc_test = []\n",
    "    for t in range(50):\n",
    "        i = reference_subset[t]\n",
    "        z_y_train = AGG_Z_Y.iloc[:,t]\n",
    "        z_y_test = Agg_z_y_test.iloc[:,t]\n",
    "        beta_y = BETA_Y[i]\n",
    "        name_beta_y = Filter(beta_y.columns, [f'in_{t+1}_iteration'])\n",
    "        target_beta_y = beta_y[name_beta_y].values\n",
    "        tau_y = TAU_Y[i]\n",
    "        name_tau_y = Filter(tau_y.columns, [f'in_{t+1}_iteration'])\n",
    "        target_tau_y = tau_y[name_tau_y].values\n",
    "        score_train = []\n",
    "        score_test = []\n",
    "        prc_ttrain = []\n",
    "        prc_ttest = []\n",
    "        for j in range(3):\n",
    "            dat_train = Training[z_y_train == j]\n",
    "            dat_test = Testing[z_y_test == j]\n",
    "            times_train = np.unique(np.exp(dat_train['Y']))\n",
    "            times_test = np.unique(np.exp(dat_test['Y']))\n",
    "            y_train1 = Surv.from_arrays(dat_train['c_y'], np.exp(dat_train['Y']))\n",
    "            y_test1 = Surv.from_arrays(dat_test['c_y'], np.exp(dat_test['Y']))\n",
    "            if times_train.max() > k and times_train.min() < k and times_test.max() > k and times_test.min() < k:\n",
    "                X_train = dat_train[X_name].values\n",
    "                X_test = dat_test[X_name].values\n",
    "                delta_train = dat_train['c_y']\n",
    "                delta_test = dat_test['c_y']\n",
    "                beta = target_beta_y[:,j] \n",
    "                mean_y_train = matmul(X_train, beta)\n",
    "                mean_y_test = matmul(X_test, beta)\n",
    "                tau = target_tau_y[:,j]\n",
    "                # pdf_train = stats.norm.pdf(k, loc=mean_y_train, scale=np.sqrt(tau[0]))\n",
    "                # pdf_test = stats.norm.pdf(k, loc=mean_y_test, scale=np.sqrt(tau[0]))\n",
    "                # cdf_train = stats.norm.cdf(k, loc=mean_y_train, scale=np.sqrt(tau[0]))\n",
    "                # cdf_test = stats.norm.cdf(k, loc=mean_y_test, scale=np.sqrt(tau[0]))\n",
    "                sf_train = stats.norm.cdf(np.log(k), loc=mean_y_train, scale=np.sqrt(tau[0]))\n",
    "                sf_test = stats.norm.cdf(np.log(k), loc=mean_y_test, scale=np.sqrt(tau[0]))\n",
    "                auroc_train = cumulative_dynamic_auc(y_train1, y_train1, sf_train, k)\n",
    "                auroc_test = cumulative_dynamic_auc(y_train1, y_test1, sf_test, k)\n",
    "                # h_train = pdf_train/sf_train\n",
    "                # h_test = pdf_test/sf_test\n",
    "                # H_train = -np.log(sf_train)\n",
    "                # H_test = -np.log(sf_train)\n",
    "                score_train.append(auroc_train[0])\n",
    "                score_test.append(auroc_test[0])\n",
    "                precision_train, recall_train, thresholds_train = metrics.precision_recall_curve(delta_train, sf_train)\n",
    "                precision_test, recall_test, thresholds_test = metrics.precision_recall_curve(delta_test,\n",
    "                                                                                                sf_test)\n",
    "                prc_train = pd.DataFrame({'precision_train': precision_train, 'recall_train': recall_train})\n",
    "                prc_test = pd.DataFrame({'precision_test': precision_test, 'recall_test': recall_test})\n",
    "                prc_train.sort_values(by='precision_train', inplace=True)\n",
    "                prc_test.sort_values(by='precision_test', inplace=True)\n",
    "                auprc_train = metrics.auc(recall_train, precision_train)\n",
    "                auprc_test = metrics.auc(recall_test, precision_test)\n",
    "                prc_ttrain.append(auprc_train)\n",
    "                prc_ttest.append(auprc_test)\n",
    "        Score_train.append(np.mean(score_train))\n",
    "        Score_test.append(np.mean(score_test))\n",
    "        Prc_train.append(np.mean(auprc_train))\n",
    "        Prc_test.append(np.mean(auprc_test))\n",
    "    SCORE_train.append(Score_train)\n",
    "    SCORE_test.append(Score_test)\n",
    "    PRC_train.append(Prc_train)\n",
    "    PRC_test.append(Prc_test)\n",
    "SCORE_train = np.array(SCORE_train)\n",
    "SCORE_test = np.array(SCORE_test)\n",
    "PRC_train = np.array(PRC_train)\n",
    "PRC_test = np.array(PRC_test)\n",
    "print(PRC_train.shape)\n",
    "# pd.DataFrame({'AUROC-Training':[np.mean(SCORE_train), np.std(SCORE_train)], 'AUROC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99889105, 0.89649387, 0.85904661, 0.94074486, 0.95445491,\n",
       "       0.96175068 ])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCORE_train.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99796804, 0.88915669, 0.84856715, 0.91294391, 0.91992174,\n",
       "       0.94783624])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCORE_test.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32721594, 0.60649115, 0.73964618 , 0.95693547, 0.97987916,\n",
       "       0.99903519])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRC_train.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33051496, 0.59039158, 0.73939994, 0.95696487, 0.98384205,\n",
       "       0.99920564])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRC_test.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_score_train = []\n",
    "CI_score_test = []\n",
    "for t in range(50):\n",
    "    i = reference_subset[t]\n",
    "    z_y_train = AGG_Z_Y.iloc[:,t]\n",
    "    z_y_test = Agg_z_y_test.iloc[:,t]\n",
    "    beta_y = BETA_Y[i]\n",
    "    name_beta_y = Filter(beta_y.columns, [f'in_{t+1}_iteration'])\n",
    "    target_beta_y = beta_y[name_beta_y].values\n",
    "    tau_y = TAU_Y[i]\n",
    "    name_tau_y = Filter(tau_y.columns, [f'in_{t+1}_iteration'])\n",
    "    target_tau_y = tau_y[name_tau_y].values\n",
    "    ci_score_train = []\n",
    "    ci_score_test = []\n",
    "    for j in range(3):\n",
    "        Z_y_t = Agg_z_y[i].loc[Agg_z_y[i][f'z_y_{t+1}_iteration'] == j,]\n",
    "        dat_train = Training[z_y_train == j]\n",
    "        dat_test = Testing[z_y_test == j]\n",
    "        X_train = dat_train[X_name].values\n",
    "        X_test = dat_test[X_name].values\n",
    "        y_train = np.exp(dat_train['Y'])\n",
    "        y_test = np.exp(dat_test['Y'])\n",
    "        delta_train = dat_train['c_y']\n",
    "        delta_test = dat_test['c_y']\n",
    "        beta = target_beta_y[:,j]\n",
    "        mean_y_train = matmul(X_train, beta)\n",
    "        mean_y_test = matmul(X_test, beta)\n",
    "        tau = target_tau_y[:,j]\n",
    "        # pdf_train = stats.norm.pdf(y_train, loc=matmul(X_train, target_beta_y[:,j]), scale=np.sqrt(tau[0]))\n",
    "        # pdf_test = stats.norm.pdf(y_test, loc=matmul(X_test, target_beta_y[:,j]), scale=np.sqrt(tau[0]))\n",
    "        # sf_train = stats.norm.sf(y_train, loc=matmul(X_train, target_beta_y[:,j]), scale=np.sqrt(tau[0]))\n",
    "        # sf_test = stats.norm.sf(y_test, loc=matmul(X_test, target_beta_y[:,j]), scale=np.sqrt(tau[0]))\n",
    "        # h_train = pdf_train/sf_train\n",
    "        # h_test = pdf_test/sf_test\n",
    "        # H_train = -np.log(sf_train)\n",
    "        # H_test = -np.log(sf_train)\n",
    "        citrain = concordance_index_censored(delta_train.astype(bool), y_train, -mean_y_train)\n",
    "        citest = concordance_index_censored(delta_test.astype(bool), y_test, -mean_y_test)\n",
    "        ci_score_train.append(citrain)\n",
    "        ci_score_test.append(citest)\n",
    "    CI_score_train.append(ci_score_train)\n",
    "    CI_score_test.append(ci_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C-index-train</th>\n",
       "      <th>C-index-test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.820680</td>\n",
       "      <td>0.829405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.024989</td>\n",
       "      <td>0.024662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      C-index-train  C-index-test\n",
       "mean       0.820680      0.829405\n",
       "std        0.024989      0.024662"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CItrain = []\n",
    "CItest = []\n",
    "for i in range(50):\n",
    "    train_correct = []\n",
    "    train_incorrect = []\n",
    "    test_correct = []\n",
    "    test_incorrect = []\n",
    "    for j in range(3):\n",
    "        est_train = CI_score_train[i][j]\n",
    "        est_test = CI_score_test[i][j]\n",
    "        correct_train = est_train[1]\n",
    "        incorrect_train = est_train[2]\n",
    "        correct_test = est_test[1]\n",
    "        incorrect_test = est_test[2]\n",
    "        train_correct.append(correct_train)\n",
    "        train_incorrect.append(incorrect_train)\n",
    "        test_correct.append(correct_test)\n",
    "        test_incorrect.append(incorrect_test)\n",
    "    citrain = sum(train_correct)/(sum(train_correct) + sum(train_incorrect))\n",
    "    citest = sum(test_correct)/(sum(test_correct) + sum(test_incorrect))\n",
    "    CItrain.append(citrain)\n",
    "    CItest.append(citest)\n",
    "\n",
    "pd.DataFrame({'C-index-train':[np.mean(CItrain), np.std(CItrain)], 'C-index-test':[np.mean(CItest), np.std(CItest)]}, index=['mean', 'std'])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2642"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
